# Quantified Self Data Project - Google and Data Tracking

**Author** Naseeb Bains
**Issue** CTRL+ALT+DH Fall 2024 - *Digital Dilemmas*
**Date** October 2025

---
Google search: the world's most popular search engine. Acting as a digital directory, Google claims to provide information regarding searched topics. Whether it’s problem solving, research, or self education, searching on Google will help you come to conclusions. The use of generative AI is a part of Google's system and it is important to know how when we use Google our data is tracked, collected, predicted, and shared. A 2020/21 survey by the Privacy Commissioner of Canada found that 81% of Canadians have “not much” or “no trust at all in social media companies to protect their information. Although google search is not a direct social media platform, it is involved with large amounts of data collection and tracking with each user. This data is then analyzed and acted on to alter individual experience using the search engine.
 Due to how deeply embedded Google is in our daily lives, and how Google’s practices in collecting, tracking, and surveilling our data are relatively hidden and mandatory, it is nearly impossible to disengage in this data-tracking platform. To expand more on the necessary data collection this essay will explore how personalization and trusting algorithmic decision making tools can work to shape our behaviour and actions, reinforce biases, and remove critical reflection from our thought processes.
 ---

 ## The rise and role of Google search

 Launched in 1998, Google search was created with the mission to “organize the world’s information and make it universally accessible and useful” (Our Approach - How Google Search Works, n.d.). Now being the world's most popular search engine and processing over 5 trillion searches annually, Google is now a near-essential tool for everyday functioning. Without it we would be stuck pondering questions, be disconnected from immediate information, and unable to participate in the extremely fast-paced, search-driven culture that shapes our modern education, work, and social environments. When we ask someone a question they do not know the answer to, we are very often told to “just google it”. This common use of language commonly seen among friends, family, peers, and educators is a reflection of how deeply ingrained Google is in the roots of our knowledge consumption, as well as a reflection of how humans often place trust in systems that we do not fully understand. Shoshana Zuboff argues that our ordinary lives are turned into the 21st century version of the “Faustian compact”, as the use of Google search results in us giving away our rights of privacy and autonomy for access and convenience to instant information (Zuboff, 2018). Through my investigation of Google’s data collection practices and privacy policies, it is evident that the true cost of this exchange (privacy+autonomy for access+information) is hidden by the platform design and shallow opt-out features.

 ---

 ## Interface design

 Google Search’s classic interface design is very minimalistic as it features a plain white background, central search bar, upper search bar, and minimal clutter. Recurring users will receive recommendations for frequently visited pages placed in the centre of the screen. Queries can be done by keyboard, microphone, and image searching, making it accessible to a wide variety of users and skill sets. The simplistic design and popularity of Google offers ease, while also encouraging user engagement without questioning how the system is operating. While it is not stated on our search results page, within settings and terms Google explains how our data determines what we see. Google search results are ranked using complex algorithms that assess the following factors: meaning of the query, relevance, quality, usability, and context (user data) (How Does Google Determine Ranking Results - Google Search, n.d.) These systems are designed to match our interests, and these interests are determined only by tracking queries and personal data. Algorithms prioritize engagement and behavioural prediction in hopes to create a “better” or “more personalized” experience, however users have little control over what data is collected and how it is used. In attempts to offer users a sense of reassurance and feeling of control, Google has “Ad Settings” and “Privacy Suggestions” tools deep within the system. Through exploration it is clear these tools are insufficient in granting users real control, as there is simply no way to opt out of data they collect. For example, turning off personalized ads does not stop Google from collecting and sharing our data, it simply stops targeted ads from being shown to you. As data collection is inevitable with the use of Google, it is important to note what is collected and why.

 ---

 ## Trust in Google search 

 Google has AI systems in place that intend to personalize our search experience, and data collection is the heart of this system. To list some of the key collected items, there is search activity, device and network information, views and interactions with ads/content, voice and audio information, purchase activity, third party activity, and browsing history (How Does Google Determine Ranking Results - Google Search, n.d.). Google's system will also guess data about an individual if it is not explicitly given, for example your gender identity. Even when given the option to not disclose your gender identity, Google will still infer it in relation to other collected data (Shekhawat et al., 2019). This data is shared with advertisers, business partners, sponsors, and other third parties - however these are broad terms with no explicit statements regarding who and what these companies are. Google expresses that these systems will not consider political leaning/viewpoint, instead using quantifiable signals (How Does Google Determine Ranking Results - Google Search, n.d.). However, seemingly neutral signals like “engagement” and “click through rates” can reproduce personal and societal biases, as well as filter opinions. 

While algorithmic produced results help individuals manage large amounts of information, our clicks, searches, and queries are not sufficient to accurately reflect the complexity of our abilities and meaning as human beings (Rouvroy, 2013). The society we live in is one where the use of platforms like search engines are an important part of our human experience; shaping how we learn, communicate, and make decisions. Not only is Google used to inquire things, but it also acts as a bridge to access many other social media platforms and digital services. I myself use Google search to access platforms like instagram, reddit, linkedin, tiktok, etc… all of which play key parts in my digital identity - yet they do not define my abilities, value, and sense of self. Through the logic of surveillance capitalism we can see how
our experiences in the digital age are no longer just our experiences, but how they are resources to be mined for data, turned into predictions, and sold for profit (Zuboff, 2018). This data is used against us in ways we often are unaware of; by nudging us to make purchases, reinforcing biases and creating information echo chambers, our physical lives and digital identities are shaped without conscious awareness.

---

## Personalized experiences

Datafication is the phenomenon which refers to the quantification of human life through digital information, typically for economic value (Mejias & Couldry, 2019). In this context, Google’s personalized ad system shows how datafication has become a key factor in surveillance capitalism. While users are led to believe opting out of personalized ads limits data collection, it simply does not. With personalized ads turned off, data including search terms, browsing behaviour, device type, location, and the time of day is still actively collected and used to determine what ads you are shown (How Personalized Ads Work - Android - My Ad Center Help, n.d.). For the purpose of this assignment I spent two weeks closely observing my ads, one week with ad personalization on and one week with it off. I found even with ad personalization off, I still received ads for shopping sites I regularly visited, even with specific styles of clothing I tend to browse. These ads were very clearly still being targeted to the persona Google had created for me as it was reflective of my browsing behaviours and collected/inferred data. With personalized ads turned on, the ads were extremely specific, down to single skincare products and bundles I have viewed and purchased in the past. While personalized ads did have a greater and more enticing impact on me, nearly convincing me to buy the $111 Caudalie serum I am trying to refrain from spending money on, targeting occurs regardless of your Ad Setting choices. Personalization based on collected data and activity becomes the default for this profit-driven agenda on a seemingly free platform.
Google tracks your clicks, searches, and inferred gender, that information is then inscribed in a collective structure - like Google’s ad system or profile database. Not only does this inscription affect your experience, but also contributes to broader predictive models used across the platform for other users. It is particularly concerning how targeting done through advertisements can reproduce and amplify social biases. A study done by N.Shekhawat, A.Chauhan, and S.Muthiah found that job related searches on Google influenced ad delivery and predictions due to biased gendered assumptions. When predicting a user's gender, Google decides based on websites searched, clicked, and visited. “Finance jobs” were more likely to target assumed male users while “yoga” and “nursing” were directed at female assigned users (Shekhawat et al., 2019). With ad personalization off and gender not voluntarily disclosed, these ads were targeted based on gender biased assumptions that are embedded in society's way of thinking. This study connects Google with Ruha Benjamin’s ideas of how seemingly neutral technologies often reproduce inequality; the New Jim Code (Benjamin, 2019). 
The personalization of ads within Google represents how datafication, data surveillance, and algorithmic governance work hand in hand. With risk of repurposing biases, our digital activity is reduced to data points that are not only collected but also acted upon in hopes of influencing our actions.
Personalized search results are a convenient feature which individuals can likely appreciate, maybe without even realizing. However, it is important to consider the ethical implications on the effects automated decision making has on our interpretation of information and knowledge. By tailoring results based on our data profiles and engagement patterns, Google narrows the scope of what we consume.
 Filter bubbles are situations where internet users often come across content that reinforces what they already think or believe; Google claims to not put people in filter bubbles as their systems do not infer sensitive characteristics like race, religion, or political affiliation (Our Approach - How Google Search Works, n.d.). However, filter bubbles are not only relevant when considering topics that involve sensitive characteristics. When we search something on Google, the algorithm decides what content is most relevant and places it near the top by combining objective accuracy with behavioural predictions. Users (myself included) have the tendency to focus within the top results of the first page, rarely digging deep into pages to locate information. Automation bias is the tendency humans have to over-rely on automated systems that we trust (Goddard et al., 2011). When we search something up, we place our trust in Google to provide us with accurate, relevant, and important information that aids us in coming to conclusions. If an individual frequently searches “Organic Food Benefits”, Google may show more results emphasizing positive aspects of organic food while downplaying criticism and counterarguments. Selective filtering used to boost user engagement can create biased understandings of complex topics and reinforce preconceived notions (Goswami, 2025). Critical thinking and reflection are skills that are necessary in well rounded information ecosystems (Goswami, 2025). By offering immediate results, source ranking distinguished by engagement metrics, and our human tendency of automation bias, our critical thinking and reflection skills are at risk to be undermined and under-practiced. As Rouvroy explains in her theory of algorithmic governmentality, algorithmic systems like Google do not ask or know who we are and what we think. Instead we can see how they infer patterns, shape our digital environments and the results to align with those predictions (Rouvroy, 2013).

 ---

 ##Summary 

 Disengaging from data tracking technologies like Google Search is not only difficult, but structurally discouraged. With Google’s normalized and extremely common use, it is important to explore how the platform's interface, governance, and use of algorithmic systems is designed to shape behaviour, maximize engagement, and profit from our data. Through exploring the platform, reflecting on my experiences, applying scholarly frameworks, and integrating scientific studies, it is evident how Google does not only collect our data, but predicts, classifies, and modifies our experiences and digital identities. While promoted as a neutral tool for information access, Google is utilized for professional profit. Raising important ethical considerations, Google can actively nudge our actions, repurpose biases, and impact our critical reflection. As humans living alongside rapid technological advancements and normalizations, we must question how power is repurposed within the systems we rely on

 ---
 
 References 

Goddard, K., Roudsari, A., & Wyatt, J. C. (2011). Automation bias: a systematic review of frequency, effect mediators, and mitigators. Journal of the American Medical Informatics Association, 19(1), 121–127. https://doi.org/10.1136/amiajnl-2011-000089
Goswami, G. (2025). AI Echo Chambers: How Algorithms Shape Reality, Influence Democracy, and Fuel Misinformation. TechRxiv. https://doi.org/10.36227/techrxiv.174059950.03385147/v1
How does Google determine ranking results - Google Search. (n.d.). How Search Works. https://www.google.com/intl/en_us/search/howsearchworks/how-search-works/ranking-results/
How personalized ads work - Android - My Ad Center Help. (n.d.). https://support.google.com/My-Ad-Center-Help/answer/12155656?hl=en&co=GENIE.Platform%3DAndroid#zippy=%2Cnon-personalized-ads-on-google
Mejias, U. A., & Couldry, N. (2019). Datafication. Internet Policy Review, 8(4). https://doi.org/10.14763/2019.4.1428
Our Approach - How Google Search Works. (n.d.). How Search Works. https://www.google.com/intl/en_us/search/howsearchworks/our-approach/#:~:text=Google's%20mission%20is%20to%20organize,height%20of%20the%20Eiffel%20Tower
Rouvroy, A. (2013). The end(s) of critique: data behaviourism versus due process. In Routledge eBooks (pp. 157–182). https://doi.org/10.4324/9780203427644-16
Shekhawat, N., Chauhan, A., & Muthiah, S. B. (2019). Algorithmic Privacy and Gender Bias Issues in Google Ad Settings. ACM DIGITAL LIBRARY, 281–285. https://doi.org/10.1145/3292522.3326033

 

 
